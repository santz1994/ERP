groups:
  - name: erp_production
    interval: 30s
    rules:

      # ============================================
      # API & Backend Alerts
      # ============================================

      # High API Latency (p95 > 1s)
      - alert: HighAPILatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="erp-backend"}[5m])) > 1.0
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High API latency detected on {{ $labels.instance }}"
          description: "API request latency p95 is {{ $value }}s (threshold: 1.0s)"
          dashboard: "https://erp.qutykarunia.com/grafana/d/api-performance"

      # High Error Rate (> 5%)
      - alert: HighErrorRate
        expr: (rate(http_requests_total{job="erp-backend",status=~"5.."}[5m]) / rate(http_requests_total{job="erp-backend"}[5m])) > 0.05
        for: 3m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      # API Service Down
      - alert: APIServiceDown
        expr: up{job="erp-backend"} == 0
        for: 1m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "API service is down"
          description: "FastAPI service {{ $labels.instance }} is not responding"

      # High Request Rate (Potential DDoS)
      - alert: HighRequestRate
        expr: rate(http_requests_total{job="erp-backend"}[1m]) > 10000
        for: 2m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "Unusually high request rate detected"
          description: "Request rate is {{ $value | humanize }}/sec (threshold: 10000/sec)"

      # ============================================
      # Database Alerts
      # ============================================

      # Database Connection Pool Exhausted (> 90%)
      - alert: DBConnectionPoolExhausted
        expr: (pg_stat_activity_count / pg_settings_max_connections) > 0.9
        for: 2m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Database connection pool usage > 90%"
          description: "PostgreSQL connection pool usage: {{ $value | humanizePercentage }}"
          recommendation: "Increase max_connections or investigate long-running queries"

      # Database Down
      - alert: PostgreSQLDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "PostgreSQL database is down"
          description: "PostgreSQL service {{ $labels.instance }} is not responding"

      # High Query Duration (> 5s)
      - alert: SlowQueries
        expr: rate(pg_slow_queries_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Slow database queries detected"
          description: "Number of slow queries (> 5s): {{ $value }}"

      # Database Disk Usage (> 80%)
      - alert: DatabaseDiskUsageHigh
        expr: (node_filesystem_avail_bytes{device=~"/dev/.*",fstype!~"tmpfs"} / node_filesystem_size_bytes) < 0.2
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Database disk usage > 80%"
          description: "Available disk space: {{ $value | humanizePercentage }}"

      # Replication Lag (if applicable)
      - alert: PostgreSQLReplicationLag
        expr: pg_replication_lag_seconds > 10
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "PostgreSQL replication lag > 10 seconds"
          description: "Current replication lag: {{ $value }}s"

      # ============================================
      # Redis Cache Alerts
      # ============================================

      # Redis Down
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
          component: cache
        annotations:
          summary: "Redis cache is down"
          description: "Redis service {{ $labels.instance }} is not responding"

      # Redis Memory Usage High (> 80%)
      - alert: RedisMemoryUsageHigh
        expr: (redis_memory_used_bytes / redis_memory_max_bytes) > 0.8
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Redis memory usage > 80%"
          description: "Redis memory usage: {{ $value | humanizePercentage }}"

      # Redis Evictions Happening
      - alert: RedisEvictionsIncreasing
        expr: rate(redis_evicted_keys_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Redis is evicting keys due to memory pressure"
          description: "Evictions rate: {{ $value }}/sec"

      # ============================================
      # QT-09 Protocol & Production Workflow
      # ============================================

      # QT-09 Line Clearance Violations
      - alert: LineClearanceViolations
        expr: rate(qt09_line_clearance_violations_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
          component: workflow
        annotations:
          summary: "QT-09 line clearance violations detected"
          description: "Violations rate: {{ $value }}/sec. Check production workflow!"
          runbook: "https://erp.qutykarunia.com/docs/qt09-violations"

      # Transfer Handshake Failures
      - alert: TransferHandshakeFailures
        expr: rate(transfer_handshake_failures_total[5m]) > 5
        for: 3m
        labels:
          severity: warning
          component: workflow
        annotations:
          summary: "High transfer handshake failure rate"
          description: "Failures: {{ $value }}/sec. Investigate transfer protocol."

      # Production Line Stalled
      - alert: ProductionLineStalled
        expr: (time() - manufacturing_order_last_update_timestamp) > 1800
        for: 30m
        labels:
          severity: warning
          component: workflow
        annotations:
          summary: "Production line {{ $labels.department }} stalled for > 30 min"
          description: "Last update: {{ $value }} seconds ago"

      # High Defect Rate
      - alert: HighDefectRate
        expr: (rate(qc_defects_total[1h]) / rate(qc_inspections_total[1h])) > 0.02
        for: 1h
        labels:
          severity: warning
          component: quality
        annotations:
          summary: "Defect rate > 2%"
          description: "Current defect rate: {{ $value | humanizePercentage }}"

      # ============================================
      # Critical Quality Control - Metal Detector
      # ============================================

      # Metal Detector Failure (CRITICAL)
      - alert: MetalDetectorFailureAlert
        expr: metal_detector_failure_alert == 1
        for: 1m
        labels:
          severity: critical
          component: quality
        annotations:
          summary: "ðŸš¨ CRITICAL: Metal detected in finished product!"
          description: "Metal detector triggered ALERT. Production STOPPED for investigation."
          action: "IMMEDIATE: Investigate finished product batch. Segregate affected units."
          runbook: "https://erp.qutykarunia.com/docs/metal-detector-incident"

      # Metal Detector False Failure Rate (> 5%)
      - alert: MetalDetectorFalseFailureRate
        expr: (rate(metal_detector_false_failures_total[1h]) / rate(metal_detector_tests_total[1h])) > 0.05
        for: 1h
        labels:
          severity: warning
          component: quality
        annotations:
          summary: "Metal detector false failure rate > 5%"
          description: "False positives: {{ $value | humanizePercentage }}. Check calibration."

      # ============================================
      # System & Infrastructure
      # ============================================

      # Node Down
      - alert: NodeDown
        expr: up{job="node"} == 0
        for: 1m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Server node {{ $labels.instance }} is down"
          description: "Node has been unresponsive for 1 minute"

      # High CPU Usage (> 80%)
      - alert: HighCPUUsage
        expr: (100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage: {{ $value | humanize }}%"

      # High Memory Usage (> 85%)
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.85
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage: {{ $value | humanizePercentage }}"

      # Disk Space Almost Full (< 10%)
      - alert: DiskSpaceAlmostFull
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs"} / node_filesystem_size_bytes) < 0.1
        for: 5m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Disk space almost full on {{ $labels.instance }}"
          description: "Available space: {{ $value | humanizePercentage }}"

      # High I/O Wait
      - alert: HighIOWait
        expr: avg by(instance) (rate(node_cpu_seconds_total{mode="iowait"}[5m])) * 100 > 20
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High I/O wait on {{ $labels.instance }}"
          description: "I/O wait: {{ $value | humanize }}%"

      # ============================================
      # Backup & Recovery Alerts
      # ============================================

      # Backup Failed
      - alert: BackupFailed
        expr: (time() - backup_last_successful_timestamp) > 86400
        for: 1m
        labels:
          severity: critical
          component: backup
        annotations:
          summary: "Database backup failed - no successful backup in 24 hours"
          description: "Last successful backup: {{ $value | humanizeDuration }} ago"
          action: "Check backup service immediately"

      # Backup Size Unusually Large
      - alert: BackupSizeAbnormal
        expr: (backup_size_bytes / avg_backup_size_bytes) > 2.0
        for: 5m
        labels:
          severity: warning
          component: backup
        annotations:
          summary: "Backup size abnormally large"
          description: "Current backup size: {{ $value | humanize }}x average"

      # ============================================
      # Prometheus & Alerting Infrastructure
      # ============================================

      # Prometheus Down
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "Prometheus monitoring system is down"

      # Alertmanager Down
      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 1m
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "Alertmanager is down - alerts may not be delivered"

      # Grafana Down
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 1m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Grafana dashboard is down"

      # Too Many Alerts
      - alert: TooManyAlerts
        expr: count(ALERTS{severity!="none"}) > 20
        for: 10m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Too many active alerts ({{ $value }})"
          description: "System may be experiencing cascading failures. Review alerts."

